#  @abstr_image eXtreme Gradient Boosting

@abstr_hyperlink @abstr_hyperlink @abstr_hyperlink @abstr_hyperlink @abstr_hyperlink @abstr_hyperlink 

@abstr_hyperlink | @abstr_hyperlink | Resources | Contributors | Release Notes

XGBoost is an optimized distributed gradient boosting library designed to be highly **_efficient_** , **_flexible_** and **_portable_**. It implements machine learning algorithms under the @abstr_hyperlink framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

## License

Â© Contributors, @abstr_number . Licensed under an @abstr_hyperlink license.

## Contribute to XGBoost

XGBoost has been developed and used by a group of active community members. Your help is very valuable to make the package better for everyone. Checkout the @abstr_hyperlink 

## Reference

  * Tianqi Chen and Carlos Guestrin. @abstr_hyperlink . In @abstr_number nd SIGKDD Conference on Knowledge Discovery and Data Mining, @abstr_number 
  * XGBoost originates from research project at University of Washington.



## Sponsors

Become a sponsor and get a logo here. See details at @abstr_hyperlink . The funds are used to defray the cost of continuous integration and testing infrastructure (https://xgboost-ci.net).

## Open Source Collective sponsors

### Sponsors

[[Become a sponsor](https://opencollective.com/xgboost#sponsor)]

@abstr_hyperlink @abstr_hyperlink @abstr_hyperlink @abstr_hyperlink @abstr_hyperlink @abstr_hyperlink @abstr_hyperlink @abstr_hyperlink @abstr_hyperlink @abstr_hyperlink 

### Backers

[[Become a backer](https://opencollective.com/xgboost#backer)]

@abstr_hyperlink 

## Other sponsors

The sponsors in this list are donating cloud hours in lieu of cash donation.

@abstr_hyperlink 
