@abstr_hyperlink   


# Apache MXNet (incubating) for Deep Learning

| Master | Docs | License | | :-------------:|:-------------:|:--------:| | @abstr_hyperlink | @abstr_hyperlink | |

@abstr_image 

Apache MXNet (incubating) is a deep learning framework designed for both _efficiency_ and _flexibility_. It allows you to **_mix_** @abstr_hyperlink to **_maximize_** efficiency and productivity. At its core, MXNet contains a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly. A graph optimization layer on top of that makes symbolic execution fast and memory efficient. MXNet is portable and lightweight, scaling effectively to multiple GPUs and multiple machines.

MXNet is more than a deep learning project. It is a collection of @abstr_hyperlink for building deep learning systems, and interesting insights of DL systems for hackers.

## Ask Questions

  * Please use @abstr_hyperlink for asking questions.
  * Please use @abstr_hyperlink for reporting bugs.
  * @abstr_hyperlink 



## How to Contribute

  * @abstr_hyperlink 



## What's New

  * @abstr_hyperlink - MXNet @abstr_number . @abstr_number . @abstr_number Patch Release.
  * @abstr_hyperlink - MXNet @abstr_number . @abstr_number . @abstr_number Release.
  * @abstr_hyperlink - MXNet @abstr_number . @abstr_number . @abstr_number Patch Release.
  * @abstr_hyperlink - MXNet @abstr_number . @abstr_number . @abstr_number Release.
  * @abstr_hyperlink - MXNet @abstr_number . @abstr_number . @abstr_number Release.
  * @abstr_hyperlink - MXNet @abstr_number . @abstr_number . @abstr_number Release.
  * @abstr_hyperlink - MXNet @abstr_number . @abstr_number . @abstr_number Release.
  * @abstr_hyperlink - MXNet @abstr_number . @abstr_number . @abstr_number Patch Release.
  * @abstr_hyperlink - MXNet @abstr_number . @abstr_number . @abstr_number Release.
  * @abstr_hyperlink - MXNet @abstr_number . @abstr_number . @abstr_number Release.
  * @abstr_hyperlink - We are now an Apache Incubator project.
  * @abstr_hyperlink - MXNet @abstr_number . @abstr_number . @abstr_number Release.
  * Version @abstr_number . @abstr_number . @abstr_number Release - First @abstr_number . @abstr_number official release.
  * Version @abstr_number . @abstr_number . @abstr_number Release (NNVM refactor) - NNVM branch is merged into master now. An official release will be made soon.
  * @abstr_hyperlink 
  * Updated Image Classification with new Pre-trained Models
  * @abstr_hyperlink 
  * MKLDNN for Faster CPU Performance
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink - Outdated
  * @abstr_hyperlink 



## Contents

  * @abstr_hyperlink and @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 



## Features

  * Design notes providing useful insights that can re-used by other DL projects
  * Flexible configuration for arbitrary computation graph
  * Mix and match imperative and symbolic programming to maximize flexibility and efficiency
  * Lightweight, memory efficient and portable to smart devices
  * Scales up to multi GPUs and distributed setting with auto parallelism
  * Support for @abstr_hyperlink , @abstr_hyperlink , @abstr_hyperlink , @abstr_hyperlink , @abstr_hyperlink , @abstr_hyperlink , @abstr_hyperlink , @abstr_hyperlink , @abstr_hyperlink , @abstr_hyperlink , and @abstr_hyperlink 
  * Cloud-friendly and directly compatible with S @abstr_number , HDFS, and Azure



## License

Licensed under an @abstr_hyperlink license.

## Reference Paper

Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. @abstr_hyperlink . In Neural Information Processing Systems, Workshop on Machine Learning Systems, @abstr_number 

## History

MXNet emerged from a collaboration by the authors of @abstr_hyperlink , @abstr_hyperlink , and @abstr_hyperlink . The project reflects what we have learned from the past projects. MXNet combines aspects of each of these projects to achieve flexibility, speed, and memory efficiency.
