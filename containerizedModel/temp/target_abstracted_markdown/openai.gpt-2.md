# gpt- @abstr_number

Code from the paper @abstr_hyperlink .

We have currently released small ( @abstr_number M parameter) and medium ( @abstr_number M parameter) versions of GPT- @abstr_number . While we have not released the larger models, we have @abstr_hyperlink for researchers to study their behaviors.

See more details in our @abstr_hyperlink .

## Usage

This repository is meant to be a starting point for researchers and engineers to experiment with GPT- @abstr_number .

### Some caveats

  * GPT- @abstr_number models' robustness and worst case behaviors are not well-understood. As with any machine-learned model, carefully evaluate GPT- @abstr_number for your use case, especially if used without fine-tuning or in safety-critical applications where reliability is important.
  * The dataset our GPT- @abstr_number models were trained on contains many texts with @abstr_hyperlink and factual inaccuracies, and thus GPT- @abstr_number models are likely to be biased and inaccurate as well.
  * To avoid having samples mistaken as human-written, we recommend clearly labeling samples as synthetic before wide dissemination. Our models are often incoherent or inaccurate in subtle ways, which takes more than a quick read for a human to notice.



### Work with us

Please @abstr_hyperlink if you’re doing interesting research with or working on applications of GPT- @abstr_number ! We’re especially interested in hearing from and potentially working with those who are studying \- Potential malicious use cases and defenses against them (e.g. the detectability of synthetic text) \- The extent of problematic content (e.g. bias) being baked into the models and effective mitigations

## Development

See DEVELOPERS.md

## Contributors

See CONTRIBUTORS.md

## Citation

Please use the following bibtex entry: @abstr_code_section 

## Future work

We may release code for evaluating the models on various benchmarks.

We are still considering release of the larger models.

## License

MIT
