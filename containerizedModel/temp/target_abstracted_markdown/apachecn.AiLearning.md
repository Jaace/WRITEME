@abstr_hyperlink   
@abstr_hyperlink @abstr_hyperlink @abstr_hyperlink 

AI learning

> Special Sponsors

@abstr_hyperlink   
---  
  
## 组织介绍

  * 我们不是 Apache 的官方组织/机构/团体，只是 Apache 技术栈（以及 AI）的爱好者！
  * 合作or侵权，请联系: apachecn@ @abstr_number .com
  * **ApacheCN - 学习机器学习群【 @abstr_number 】**



> **欢迎任何人参与和完善：一个人可以走的很快，但是一群人却可以走的更远**

# 路线图

@abstr_hyperlink 

https://github.com/apachecn/ai-roadmap/blob/master/ai-union- @abstr_number /README.md

按照步骤: @abstr_number => @abstr_number => @abstr_number ，你可以当大牛！

## @abstr_number .机器学习 - 基础

  * **Machine Learning in Action (机器学习实战) | @abstr_hyperlink**
  * 电子版书籍： @abstr_hyperlink 
  * \-- 感谢 @abstr_hyperlink 生成的电子书 @abstr_hyperlink 
  * **视频已更新完成，如果你觉得有价值，请帮忙点 Star【后续组织学习活动：sklearn、kaggle、 Pytorch 和 tensorflow】**
  * \-- 视频网站：优酷 ／bilibili / Acfun / 网易云课堂，可直接在线播放。（最下方有相应链接）
  * \-- @abstr_hyperlink : @abstr_hyperlink 
  * 推荐一个 @abstr_hyperlink : https://feisky.xyz/machine-learning

模块 | 章节 | 类型 | 负责人(GitHub) | QQ  
---|---|---|---|---  
机器学习实战 |  第 @abstr_number 章: 机器学习基础 | 介绍 |  @abstr_hyperlink  |  @abstr_number   
机器学习实战 | 第 @abstr_number 章: KNN 近邻算法 | 分类 |  @abstr_hyperlink  |  @abstr_number   
机器学习实战 | 第 @abstr_number 章: 决策树 | 分类 |  @abstr_hyperlink  |  @abstr_number   
机器学习实战 | 第 @abstr_number 章: 朴素贝叶斯 | 分类 |  @abstr_hyperlink   
@abstr_hyperlink  |  @abstr_number   
@abstr_number   
机器学习实战 | 第 @abstr_number 章: Logistic回归 | 分类 |  @abstr_hyperlink  |  @abstr_number   
机器学习实战 | 第 @abstr_number 章: SVM 支持向量机 | 分类 |  @abstr_hyperlink  |  @abstr_number   
网上组合内容 | 第 @abstr_number 章: 集成方法（随机森林和 AdaBoost） | 分类 |  @abstr_hyperlink  |  @abstr_number   
机器学习实战 | 第 @abstr_number 章: 回归 | 回归 |  @abstr_hyperlink  |  @abstr_number   
机器学习实战 | 第 @abstr_number 章: 树回归 | 回归 |  @abstr_hyperlink  |  @abstr_number   
机器学习实战 | 第 @abstr_number 章: K-Means 聚类 | 聚类 |  @abstr_hyperlink  |  @abstr_number   
机器学习实战 | 第 @abstr_number 章: 利用 Apriori 算法进行关联分析 | 频繁项集 |  @abstr_hyperlink  |  @abstr_number   
机器学习实战 | 第 @abstr_number 章: FP-growth 高效发现频繁项集 | 频繁项集 |  @abstr_hyperlink  |  @abstr_number   
机器学习实战 | 第 @abstr_number 章: 利用 PCA 来简化数据 | 工具 |  @abstr_hyperlink  |  @abstr_number   
机器学习实战 | 第 @abstr_number 章: 利用 SVD 来简化数据 | 工具 |  @abstr_hyperlink  |  @abstr_number   
机器学习实战 | 第 @abstr_number 章: 大数据与 MapReduce | 工具 |  @abstr_hyperlink  |  @abstr_number   
Ml项目实战 | 第 @abstr_number 章: 推荐系统（已迁移） | 项目 |  @abstr_hyperlink  |   
第一期的总结 |  @abstr_number - @abstr_number - @abstr_number : 第一期的总结 | 总结 | 总结 |  @abstr_number   
  
### 网站视频

> @abstr_hyperlink 

当然我知道，第一句就会被吐槽，因为科班出身的人，不屑的吐了一口唾沫，说傻X，还评论 Andrew Ng 的视频。。

我还知道还有一部分人，看 Andrew Ng 的视频就是看不懂，那神秘的数学推导，那迷之微笑的英文版的教学，我何尝又不是这样走过来的？？ 我的心可能比你们都痛，因为我在网上收藏过上 @abstr_number 部《机器学习》相关视频，外加国内本土风格的教程： @abstr_number 月+小象 等等，我都很难去听懂，直到有一天，被一个百度的高级算法分析师推荐说：《机器学习实战》还不错，通俗易懂，你去试试？？

我试了试，还好我的Python基础和调试能力还不错，基本上代码都调试过一遍，很多高大上的 "理论+推导"，在我眼中变成了几个 "加减乘除+循环"，我想这不就是像我这样的程序员想要的入门教程么？

很多程序员说机器学习 TM 太难学了，是的，真 TM 难学，我想最难的是：没有一本像《机器学习实战》那样的作者愿意以程序员 Coding 角度去给大家讲解！！

最近几天，GitHub 涨了 @abstr_number 颗 star，加群的 @abstr_number 人， 现在还在不断的增加++，我想大家可能都是感同身受吧！

很多想入门新手就是被忽悠着收藏收藏再收藏，但是最后还是什么都没有学到，也就是"资源收藏家"，也许新手要的就是 @abstr_hyperlink 。没错，我可以给你们的一份，因为我们还通过视频记录下来我们的学习过程。水平当然也有限，不过对于新手入门，绝对没问题，如果你还不会，那算我输！！

> 视频怎么看？

@abstr_image 

@abstr_number . 理论科班出身-建议去学习 Andrew Ng 的视频（Ng 的视频绝对是权威，这个毋庸置疑） @abstr_number . 编码能力强 - 建议看我们的 @abstr_hyperlink @abstr_number . 编码能力弱 - 建议看我们的 @abstr_hyperlink ，不过在看理论的时候，看 教学版-理论部分；讨论版的废话太多，不过在讲解代码的时候是一行一行讲解的；所以，根据自己的需求，自由的组合。

> 【免费】数学教学视频 - 可汗学院 入门篇

  * @于振梓 推荐: 可汗学院-网易公开课



| 概率 | 统计 | 线性代数 | | - | - | - | | @abstr_hyperlink | @abstr_hyperlink | @abstr_hyperlink 

> 机器学习视频 - ApacheCN 教学版

||| | - | - | | AcFun | B站 | | @abstr_hyperlink | @abstr_hyperlink | | 优酷 | 网易云课堂 | | @abstr_hyperlink | @abstr_hyperlink |

> 【免费】机器/深度学习视频 - 吴恩达

| 机器学习 | 深度学习 | | - | - | | @abstr_hyperlink | @abstr_hyperlink |

## @abstr_number .深度学习 - 基础

> 深度学习必学

@abstr_number . 反向传递: https://www.cnblogs.com/charlotte @abstr_number /p/ @abstr_number .html @abstr_number . CNN原理: http://www.cnblogs.com/charlotte @abstr_number /p/ @abstr_number .html @abstr_number . RNN原理: https://blog.csdn.net/qq_ @abstr_number /article/details/ @abstr_number @abstr_number . LSTM原理: https://blog.csdn.net/weixin_ @abstr_number /article/details/ @abstr_number 

## @abstr_number .自然语言处理

学习过程中-内心复杂的变化！！！

@abstr_code_section 

@abstr_image 

  * **入门教程必看资料【添加比赛链接】: https://github.com/apachecn/AiLearning/tree/master/docs/nlp**
  * Python 自然语言处理 第二版: https://usyiyi.github.io/nlp-py- @abstr_number e-zh
  * 推荐一个 @abstr_hyperlink 整理的nlp全面知识体系: @abstr_hyperlink 
  * 开源 - 词向量库集合: https://github.com/Embedding/Chinese-Word-Vectors



### @abstr_number .使用场景 （百度公开课）

> 第一部分 入门介绍

  * @abstr_number .) 自然语言处理入门介绍



> 第二部分 机器翻译

  * @abstr_number .) 机器翻译



> 第三部分 篇章分析

  * @abstr_number . @abstr_number .) 篇章分析-内容概述
  * @abstr_number . @abstr_number .) 篇章分析-内容标签
  * @abstr_number . @abstr_number .) 篇章分析-情感分析
  * @abstr_number . @abstr_number .) 篇章分析-自动摘要



> 第四部分 UNIT-语言理解与交互技术

  * @abstr_number .) UNIT-语言理解与交互技术



### 应用领域

#### 中文分词：

  * 构建DAG图
  * 动态规划查找，综合正反向（正向加权反向输出）求得DAG最大概率路径
  * 使用了SBME语料训练了一套 HMM + Viterbi 模型，解决未登录词问题



#### @abstr_number .文本分类（Text Classification）

文本分类是指标记句子或文档，例如电子邮件垃圾邮件分类和情感分析。

下面是一些很好的初学者文本分类数据集。

@abstr_number . @abstr_hyperlink （路透社- @abstr_number ）。 @abstr_number 年路透社出现的一系列新闻文件，按类别编制索引。 @abstr_hyperlink 。 @abstr_number . @abstr_hyperlink 。来自网站imdb.com的一系列电影评论及其积极或消极的情绪。 @abstr_number . @abstr_hyperlink 。来自网站imdb.com的一系列电影评论及其积极或消极的情绪。

有关更多信息，请参阅帖子： @abstr_hyperlink 。

> 情感分析

比赛地址: https://www.kaggle.com/c/word @abstr_number vec-nlp-tutorial

  * 方案一( @abstr_number . @abstr_number )：WordCount + 朴素 Bayes
  * 方案二( @abstr_number . @abstr_number )：LDA + 分类模型（knn/决策树/逻辑回归/svm/xgboost/随机森林） 
    * a) 决策树效果不是很好，这种连续特征不太适合的
    * b) 通过参数调整 @abstr_number 个topic，信息量保存效果较优（计算主题）
  * 方案三( @abstr_number . @abstr_number )：word @abstr_number vec + CNN 
    * 说实话：没有一个好的机器，是调不出来一个好的结果 (: 逃



**通过AUC 来评估模型的效果**

#### @abstr_number .语言模型（Language Modeling）

语言建模涉及开发一种统计模型，用于预测句子中的下一个单词或一个单词中的下一个单词。它是语音识别和机器翻译等任务中的前置任务。

它是语音识别和机器翻译等任务中的前置任务。

下面是一些很好的初学者语言建模数据集。

@abstr_number . @abstr_hyperlink ，一系列免费书籍，可以用纯文本检索各种语言。 @abstr_number . 还有更多正式的语料库得到了很好的研究; 例如： @abstr_hyperlink 。大量英语单词样本。 @abstr_hyperlink 。

> 新词发现

  * 中文分词新词发现
  * python @abstr_number 利用互信息和左右信息熵的中文分词新词发现
  * @abstr_hyperlink 



> 句子相似度识别

  * 项目地址: https://www.kaggle.com/c/quora-question-pairs
  * 解决方案: word @abstr_number vec + Bi-GRU



> 文本纠错

  * bi-gram + levenshtein



#### @abstr_number .图像字幕（Image Captioning）

mage字幕是为给定图像生成文本描述的任务。

下面是一些很好的初学者图像字幕数据集。

@abstr_number . @abstr_hyperlink 。包含超过 @abstr_number 万张带描述的图像的集合 @abstr_number . @abstr_hyperlink 。从flickr.com获取的 @abstr_number 千个描述图像的集合。 @abstr_number . @abstr_hyperlink 。从flickr.com获取的 @abstr_number 万个描述图像的集合。 欲了解更多，请看帖子：

@abstr_hyperlink 

#### @abstr_number .机器翻译（Machine Translation）

机器翻译是将文本从一种语言翻译成另一种语言的任务。

下面是一些很好的初学者机器翻译数据集。

@abstr_number . @abstr_hyperlink 。成对的英语和法语句子。 @abstr_number . @abstr_hyperlink 。句子对一套欧洲语言。 有大量标准数据集用于年度机器翻译挑战; 看到：

@abstr_hyperlink 

> 机器翻译

  * Encoder + Decoder(Attention)
  * 参考案例: http://pytorch.apachecn.org/cn/tutorials/intermediate/seq @abstr_number seq_translation_tutorial.html



#### @abstr_number .问答系统（Question Answering）

问答是一项任务，其中提供了一个句子或文本样本，从中提出问题并且必须回答问题。

下面是一些很好的初学者问题回答数据集。

@abstr_number . @abstr_hyperlink 。回答有关维基百科文章的问题。 @abstr_number . @abstr_hyperlink 。从每日邮报回答有关新闻文章的问题。 @abstr_number . @abstr_hyperlink 。回答有关亚马逊产品的问题。 有关更多信息，请参阅帖子：

@abstr_hyperlink 

#### @abstr_number .语音识别（Speech Recognition）

语音识别是将口语的音频转换为人类可读文本的任务。

下面是一些很好的初学者语音识别数据集。

@abstr_number . @abstr_hyperlink 。不是免费的，但因其广泛使用而上市。口语美国英语和相关的转录。 @abstr_number . @abstr_hyperlink 。用于构建用于语音识别的开源数据库的项目。 @abstr_number . @abstr_hyperlink 。从LibriVox收集的大量英语有声读物。

#### @abstr_number .自动文摘（Document Summarization）

文档摘要是创建较大文档的简短有意义描述的任务。

下面是一些很好的初学者文档摘要数据集。

@abstr_number . @abstr_hyperlink 。收集了 @abstr_number 份法律案件及其摘要。 @abstr_number . @abstr_hyperlink 。收集了近 @abstr_number 份文件及其摘要。 @abstr_number . @abstr_hyperlink 。不是免费的，而是广泛使用的。新闻文章的语料库。 欲了解更多信息：

@abstr_hyperlink 。 @abstr_hyperlink 

> 命名实体识别

  * Bi-LSTM CRF
  * 参考案例: http://pytorch.apachecn.org/cn/tutorials/beginner/nlp/advanced_tutorial.html
  * CRF推荐文档: https://www.jianshu.com/p/ @abstr_number fc @abstr_number b @abstr_number 



> 文本摘要

  * **抽取式**
  * word @abstr_number vec + textrank
  * word @abstr_number vec推荐文档: https://www.zhihu.com/question/ @abstr_number /answer/ @abstr_number 
  * textrank推荐文档: https://blog.csdn.net/BaiHuaXiu @abstr_number /article/details/ @abstr_number 



## Graph图计算【慢慢更新】

  * 数据集: data/nlp/graph
  * 学习资料: spark graphX实战.pdf 【文件太大不方便提供，自己百度】



### 进一步阅读

如果您希望更深入，本节提供了其他数据集列表。

@abstr_number . @abstr_hyperlink @abstr_number . @abstr_hyperlink @abstr_number . @abstr_hyperlink @abstr_number . @abstr_hyperlink @abstr_number . @abstr_hyperlink @abstr_number . @abstr_hyperlink @abstr_number . @abstr_hyperlink @abstr_number . 国内开放数据集: https://bosonnlp.com/dev/resource

## 项目负责人

> Ml 第一期 ( @abstr_number - @abstr_number - @abstr_number )

  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_number - @abstr_number - @abstr_number _第一期的总结



> Ml 第二期 ( @abstr_number - @abstr_number - @abstr_number )

  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 



> Ml 第三期 ( @abstr_number - @abstr_number - @abstr_number )

## 项目贡献者

> Ml 第一期 ( @abstr_number - @abstr_number - @abstr_number )

  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 



> Ml 第二期 ( @abstr_number - @abstr_number - @abstr_number )

  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 



> Ml 第三期 ( @abstr_number - @abstr_number - @abstr_number )

## 群管理员换届

  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @abstr_hyperlink 
  * @LAMDA-健忘症 永久留任-非常感谢对群的贡献



> Ml 第一届 ( @abstr_number - @abstr_number - @abstr_number )

  * @易漠
  * @abstr_hyperlink 
  * @Books
  * @李孟禹
  * @张假飞
  * @Glassy
  * @红色石头
  * @微光同尘



> Ml 第二届 ( @abstr_number - @abstr_number - @abstr_number )

  * @张假飞
  * @李孟禹
  * @小明教主
  * @平淡的天
  * @凌少skierゞ
  * @じ☆νЁ坐看云起
  * 古柳-DesertsX
  * woodchuck
  * 自由精灵
  * 楚盟
  * @abstr_number 杆清台
  * 时空守望者@
  * 只想发论文的渣渣
  * 目标: ml劝退专家



> Ml 第三届 ( @abstr_number - @abstr_number - @abstr_number )

  * 只会喊 @abstr_number 的存在
  * codefun @abstr_number .xyz
  * 荼靡
  * 大鱼
  * 青鸟
  * 古柳-DesertsX
  * Edge
  * Alluka
  * 不发篇paper不改名片
  * FontTian
  * Bigjing
  * 仁 礼 智 爱
  * 可啪的小乖受
  * 老古董
  * 时空守望者
  * 我好菜啊
  * Messi @abstr_number 



> Ml 第四届 ( @abstr_number - @abstr_number - @abstr_number )

  * 佛学爱好者
  * 楚盟
  * codefun @abstr_number .xyz
  * 大鱼-群花-声优
  * 大海
  * Edge
  * if only
  * 李孟禹
  * 两年半的练习生
  * 萌Jay小公举
  * 平静
  * 任务做不完
  * 仁礼智爱
  * 园时空守望者@
  * 田丰(FontTian)
  * 坐看云起
  * Bejinger
  * 阿花君霸占路人
  * 烦焖鸡
  * 古柳-DesertsX
  * 青鸟(服务员)
  * 小明教主
  * zhiqing



**欢迎贡献者不断的追加**

## 免责声明 - 【只供学习参考】

  * ApacheCN 纯粹出于学习目的与个人兴趣翻译本书
  * ApacheCN 保留对此版本译文的署名权及其它相关权利



* * *

资料来源:

  * 【比赛收集平台】: https://github.com/iphysresearch/DataSciComp
  * https://github.com/pbharrin/machinelearninginaction
  * https://machinelearningmastery.com/datasets-natural-language-processing



## 赞助我们

@abstr_image 

## **协议**

以各项目协议为准。

ApacheCN 账号下没有协议的项目，一律视为 @abstr_hyperlink 。
